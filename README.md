# ğŸ“Š Data Pipeline Project â€“ Glue + Airflow + Iceberg

## Objectif
Ingestion de transactions.csv et tiers.csv, transformation PySpark, orchestration Airflow MWAA, sauvegarde en Apache Iceberg dans S3.

## ğŸ“ Arborescence
```
project-data-pipeline/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ main_dag.py
â”œâ”€â”€ glue_jobs/
â”‚   â”œâ”€â”€ job_ingest.py
â”‚   â””â”€â”€ job_transform.py
â”œâ”€â”€ terraform/
â”‚   â””â”€â”€ main.tf
â”œâ”€â”€ README.md
```

## ğŸ”§ Ã‰tapes locales
1. Cloner le projet :
```bash
git clone https://github.com/ton-user/project-data-pipeline.git
cd project-data-pipeline
```

2. Ã‰crire les fichiers `transactions.csv` et `tiers.csv` dans S3 bucket : `s3://your-bucket/raw/`

3. Modifier les chemins dans les scripts Glue (`glue_jobs/*.py`)

4. DÃ©ployer les ressources AWS avec Terraform :
```bash
cd terraform
terraform init
terraform apply
```

5. DÃ©ployer les scripts PySpark dans S3 :
```bash
aws s3 cp ../glue_jobs/job_ingest.py s3://your-bucket/scripts/
aws s3 cp ../glue_jobs/job_transform.py s3://your-bucket/scripts/
```

6. DÃ©ployer le DAG sur MWAA (Airflow managÃ© AWS) :
```bash
aws s3 cp ../dags/main_dag.py s3://your-airflow-dag-bucket/dags/
```

7. VÃ©rifier dans MWAA et exÃ©cuter le DAG.

## âœ… RÃ©sultat attendu
- Fichier Iceberg : `s3://your-bucket/gold/transactions_iceberg/`
- Suivi orchestration dans Airflow (MWAA)